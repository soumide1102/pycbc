#! /usr/bin/env python
# Copyright (C) 2017 Soumi De
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
""" Plots the Kullback-Leibler divergence diagnositic statistic.
"""

import argparse
import logging
from pycbc import results
from pycbc.inference import kullback_leibler
from pycbc.inference import option_utils
import matplotlib.pyplot as plt
import sys
import numpy
import math

# command line usage
parser = argparse.ArgumentParser(usage=__file__ + " [--options]",
                                 description=__doc__)

# verbose option
parser.add_argument("--verbose", action="store_true", default=False,
                    help="Print logging info.")

# output options
parser.add_argument("--output-file", type=str, required=True,
                    help="Path to output plot.")
parser.add_argument("--walkers", type=int, nargs="+", default=None,
                    help="Specific walkers to plot. Default is plot "
                         "all walkers.")
parser.add_argument("--start-iter", type=int, required=True,
                    help="Index in chain to start calculation.")
parser.add_argument("--end-iter", type=int, required=True,
                    help="Index in chain to end calculation.")
parser.add_argument("--step-iter", type=int, required=True,
                    help="Step size in chain to next calculation.")

# add results group
option_utils.add_inference_results_option_group(parser)

# parse the command line
opts = parser.parse_args()

# setup log
if opts.verbose:
    log_level = logging.DEBUG
else:
    log_level = logging.WARN
logging.basicConfig(format="%(asctime)s : %(message)s", level=log_level)

# ensure that this is not a single iteration
if opts.iteration is not None:
    raise ValueError("Cannot use --iteration")

# load the results
fp, params, labels, _ = option_utils.results_from_cli(
                                         opts, load_samples=False,
                                         walkers=None)

# if use wants specific walkers
walkers = range(fp.nwalkers) if opts.walkers is None else opts.walkers
nwalkers = len(walkers)

# create Figure
fig = plt.figure()

# loop over parameters
for param, label in zip(params, labels):
    logging.info("Plotting parameter %s", param)

    # loop over walkers
    for j in walkers:
        logging.info("Plotting walker %d of %d", j, nwalkers)

         
        # get likelihood stats for each chain
        likelihood_stats = fp.read_likelihood_stats(walkers=j,
                              thin_start=opts.thin_start,
                              thin_interval=opts.thin_interval,
                              thin_end=opts.thin_end)

        # get the logposterior values for each chain
        logposterior, _ = option_utils.get_zvalues(fp, 'logposterior', likelihood_stats)

        # get the logprior values
        logprior = likelihood_stats.prior

        # Rescale the logposterior values because those are very small
        # negative numbers. Taking the exp to get the posteriors returns 0.'s.
        # Rescaling the logprior 
        
        #posterior=numpy.zeros(len(logposterior))
        #prior=numpy.zeros(len(logprior))
        #for i in range(len(logposterior)):
        #    logposterior[i]=math.ceil(logposterior[i]/1000.0)*1000.0
        #    logprior[i]=math.ceil(logprior[i]/1000)*1000.0
        #    posterior[i] = numpy.exp(logposterior[i])
        #    posterior[i] = round(posterior[i], 16)
        #    prior[i] = numpy.exp(logprior[i])
        #    print(".16%f", posterior[i])
        #logposterior=logposterior + numpy.log(10.0**500)
        #logprior=logprior + numpy.log(10.0**500)

        #logposterior=logposterior/1000
        #logprior=logprior/1000
        #posterior=numpy.exp(logposterior)
        #prior=numpy.exp(logprior)
        ends, stats = kullback_leibler.kullback_leibler(numpy.array(logposterior),
                                                        numpy.array(logprior),
                                                        opts.start_iter,
                                                        opts.end_iter,
                                                        opts.step_iter)

        #stats = 1000*stats
        # plot
        print("ends")
        print ends
        print("stats")
        print stats
        plt.plot(ends, stats, label=label)
          

# format plot
plt.ylabel("Kullback-Leibler divergence")
plt.xlabel("Iteration")
plt.legend(labelspacing=0.2)

# save figure with meta-data
caption_kwargs = {
    "parameters" : ", ".join(labels),
}
caption = """The Kullback-Leibler divergence diagnostic statistic for {parameters}
read from the input file.""".format(**caption_kwargs)
title = "Kullback-Leibler Divergence for {parameters}".format(**caption_kwargs)
results.save_fig_with_metadata(fig, opts.output_file,
                               cmd=" ".join(sys.argv),
                               title=title,
                               caption=caption)
plt.close()

# exit
fp.close()
logging.info("Done")
